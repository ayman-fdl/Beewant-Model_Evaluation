{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://beewant.com/api/new-chat\"\n",
    "token_key = \"89b19faabe91de4df9025c61bf06dc9029222796\"\n",
    "headers = {\n",
    "    'Authorization': f'Token {token_key}',  \n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "CHAT_MODEL = 'qwen3'\n",
    "EVALUATION_MODELS = ['claude-3.7', 'GPT4.1']\n",
    "EVALUATION_DIR = 'evaluation_web'\n",
    "os.makedirs(EVALUATION_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation WEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_questions_from_csv(file_path: str):\n",
    "    questions = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file, delimiter=';')\n",
    "        for row in reader:\n",
    "            questions.append({\n",
    "                'thematic': row['thematic'],\n",
    "                'question': row['question']\n",
    "            })\n",
    "    return questions\n",
    "\n",
    "def export_json(result, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(result, indent=2))\n",
    "\n",
    "def send_question(question):\n",
    "    # You can change this to \"GPT4.1\", \"mistral\", \"llama3.3\", \"claude-3.7\", \"r1\", \"qwen3\"\n",
    "\n",
    "    data = {\n",
    "        \"prompt\": question,\n",
    "        \"chat_model\": CHAT_MODEL,\n",
    "        \"web\": True,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    for line in (response.text).splitlines():\n",
    "        last_message = None\n",
    "        try:\n",
    "            parsed = json.loads(line)\n",
    "            if 'citations' in parsed:\n",
    "                citations = parsed['citations']\n",
    "            if 'message' in parsed:\n",
    "                last_message = parsed['message']\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    links = []\n",
    "    for link in citations:\n",
    "        links.append((link['url'], link['snippet'], link['title']))\n",
    "    message = last_message\n",
    "    if '</think>' in last_message:\n",
    "        message = last_message.split(\"</think>\")[1].strip()\n",
    "    return links, message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response Evaluation:\n",
    "1. Relevance (0.4): Does the response address the question directly and stay on topic? \n",
    "2. Correctness (0.2): Is the information factually accurate?\n",
    "3. Completeness (0.15): Does it cover all the key aspects expected in the answer?\n",
    "4. Clarity (0.15): Is it clearly written and easy to understand?\n",
    "5. Depth (0.1): Does it show deep insight or thoughtful reasoning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sources(question, sources):\n",
    "    evaluation_prompt = f\"\"\"Evaluate the following sources for the question: \"{question}\"\n",
    "Sources:\n",
    "{sources}\n",
    "\n",
    "Evaluate each source based on:\n",
    "- The domain name (e.g., .org, .gov, known institutions, major news outlets),\n",
    "- The structure and keywords in the URL (e.g., presence of date or topic),\n",
    "- Any general knowledge you have about the source.\n",
    "\n",
    "For each link, assign a **total score out of 15** based on the following 3 criteria (each scored from 0 to 5):\n",
    "1. **relevance** to the user question  \n",
    "2. **credibility** of the domain  \n",
    "3. **freshness** (based on year or indicators in the URL)\n",
    "\n",
    "Add the scores for each URL and return the result as a JSON list with this structure:\n",
    "\n",
    "[\n",
    "    {{\n",
    "    \"url\": \"https://example.com/article-sahel-water-2023\",\n",
    "    \"relevance\": 2,\n",
    "    \"credibility\": 2,\n",
    "    \"freshness\": 1\n",
    "    }},\n",
    "    {{\n",
    "    \"url\": \"https://oldnews.net/blog123\",\n",
    "    \"relevance\": 3,\n",
    "    \"credibility\": 1,\n",
    "    \"freshness\": 1\n",
    "    }},\n",
    "    {{\n",
    "    \"url\": \"https://un.org/sahel-water-report\",\n",
    "    \"relevance\": 2,\n",
    "    \"credibility\": 2,\n",
    "    \"freshness\": 1\n",
    "    }}\n",
    "]\n",
    "Return only the raw JSON array. Do not include any explanation, commentary, or text outside the JSON. Do not wrap it in code blocks.\"\"\"\n",
    "\n",
    "    data = {\n",
    "        \"prompt\": evaluation_prompt,\n",
    "        \"chat_model\": EVALUATION_MODELS[0],\n",
    "        \"stream\": True\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        for line in (response.text).splitlines():\n",
    "            last_message = None\n",
    "            parsed = json.loads(line)\n",
    "            if 'message' in parsed:\n",
    "                last_message = parsed['message']\n",
    "        return last_message\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating sources: {e}\")\n",
    "        return {}\n",
    "\n",
    "def evaluate_response(question, response, model):\n",
    "    prompt = f\"\"\"You are a strict evaluator assessing the quality of a response generated by a language model on a topic-specific question. Your job is to score the response based on the following five criteria, each from 1 (very poor) to 5 (excellent):\n",
    "\n",
    "1. Relevance: Does the response address the question directly and stay on topic?\n",
    "2. Correctness: Is the information factually accurate?\n",
    "3. Completeness: Does it sufficiently cover the key aspects of the question?\n",
    "4. Clarity: Is the response clearly written and easy to understand?\n",
    "5. Depth: Does the response provide meaningful insight or analysis beyond surface-level information?\n",
    "\n",
    "After rating, provide a **very brief comment** summarizing the strengths or weaknesses in 1-2 sentences.\n",
    "Evaluate the following question and response:\n",
    "\n",
    "[QUESTION]\n",
    "{question}\n",
    "\n",
    "[RESPONSE]\n",
    "{response}\n",
    "Return the result strictly in the following JSON format (no other text):\n",
    "\n",
    "{{\n",
    "    \"Relevance\": <1-5>,\n",
    "    \"Correctness\": <1-5>,\n",
    "    \"Completeness\": <1-5>,\n",
    "    \"Clarity\": <1-5>,\n",
    "    \"Depth\": <1-5>,\n",
    "    \"Comment\": \"<short comment here>\"\n",
    "}}\n",
    "Return only the raw JSON array. Do not include any explanation, commentary, or text outside the JSON. Do not wrap it in code blocks.\n",
    "Return **only a valid Python dictionary** (no additional text or explanation) in the following format:\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"chat_model\": model,\n",
    "        \"stream\": True\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        for line in (response.text).splitlines():\n",
    "            last_message = None\n",
    "            parsed = json.loads(line)\n",
    "            if 'message' in parsed:\n",
    "                last_message = parsed['message']\n",
    "        return last_message\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating sources: {e}\")\n",
    "        return {}\n",
    "\n",
    "def eval_response_per_model(question, response):\n",
    "    evaluation = []\n",
    "    for model in EVALUATION_MODELS:\n",
    "        result = evaluate_response(question, response, model)\n",
    "        result = json.loads(result)\n",
    "        result['model'] = model\n",
    "        evaluation.append(result)\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What are the most visited countries in 2025 and what sustainable tourism practices are being promoted?\"\n",
    "# question = \"What were the root causes and geopolitical consequences of the Ukraine war between 2022 and 2025?\"\n",
    "# response = []\n",
    "# for model in EVALUATION_MODELS:\n",
    "#     result = evaluate_response(question, message, model)\n",
    "#     result = json.loads(result)\n",
    "#     result['model'] = model\n",
    "#     response.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"result2.json\", 'w', encoding='utf-8') as f:\n",
    "#     json.dump(response, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: (Religion) How are religious institutions responding to increasing secularism in Europe and North America in 2025?\n",
      "Getting links and message\n",
      "Evaluation sources\n",
      "Evaluation response\n",
      "Question: (Environment & Climate) What are the most impactful climate adaptation projects launched after COP29, and how are they being funded?\n",
      "Getting links and message\n",
      "Evaluation sources\n",
      "Evaluation response\n",
      "Question: (Social Issues) What are the main challenges and innovations in addressing global housing inequality in 2025?\n",
      "Getting links and message\n",
      "Evaluation sources\n",
      "Evaluation response\n",
      "Question: (Psychology & Mental Health) How has the rise of virtual therapy and mental health apps affected treatment accessibility and effectiveness worldwide?\n",
      "Getting links and message\n",
      "Evaluation sources\n",
      "Evaluation response\n",
      "Question: (Media & Entertainment) How is the entertainment industry adapting to AI-generated content in 2025, and what legal issues are emerging?\n",
      "Getting links and message\n",
      "Evaluation sources\n",
      "Evaluation response\n",
      "Question: (Travel & Tourism) What are the most visited countries in 2025 and what sustainable tourism practices are being promoted?\n",
      "Getting links and message\n",
      "Evaluation sources\n",
      "Evaluation response\n"
     ]
    }
   ],
   "source": [
    "questions = read_questions_from_csv(\"thematic_subjects.csv\")\n",
    "for qst in questions:\n",
    "    evaluation = {}\n",
    "    print(f\"Question: ({qst['thematic']}) {qst['question']}\")\n",
    "    links, message = send_question(question=qst['question'])\n",
    "    print(\"Getting links and message\")\n",
    "    eval_sources = evaluate_sources(qst['question'], links)\n",
    "    evaluation['sources'] = json.loads(eval_sources)\n",
    "    print(\"Evaluation sources\")\n",
    "    \n",
    "    eval_response = eval_response_per_model(qst['question'], message)\n",
    "    evaluation['response'] = eval_response\n",
    "    print(\"Evaluation response\")\n",
    "    export_json(evaluation,  f\"{EVALUATION_DIR}/Evaluation_{qst['thematic']}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "prompt = \"\"\"Answer the following 15 questions based on the content of the book. If the book doesn’t provide an answer, reply with \"Not covered in the book.\"\n",
    "\n",
    "Return your answers as a **Python list of dictionaries**, where each dictionary has the keys:\n",
    "- `\"question\"`: the original question\n",
    "- `\"answer\"`: the response from the book's content\n",
    "\n",
    "Here are the questions:\n",
    "\n",
    "1. How can I automate sending emails using Python?\n",
    "2. What is the use of the `pyautogui` module?\n",
    "3. How does Python handle copying and moving files automatically?\n",
    "4. What is the difference between `input()` and `sys.argv` for input?\n",
    "5. How can I extract data from an Excel spreadsheet using Python?\n",
    "6. What does this code snippet do? `for i in range(5): print(i)`\n",
    "7. How do I handle exceptions using try/except in Python?\n",
    "8. What’s the purpose of the `open()` function in reading files?\n",
    "9. What are the benefits of automating tasks with Python?\n",
    "10. When should I use regular expressions in automation?\n",
    "11. How does scheduling a script to run daily work on Windows?\n",
    "12. What are some examples of boring tasks that can be automated?\n",
    "13. In which chapter is web scraping introduced?\n",
    "14. Which module is recommended for manipulating PDFs?\n",
    "15. How is the mouse controlled programmatically in the book?\n",
    "\n",
    "\n",
    "Return only a valid Python list of 15 dictionaries.\"\"\"\n",
    "\n",
    "data = {\n",
    "    'prompt': prompt,\n",
    "    'media_type': 'collection',\n",
    "    'mediaID': 236,  \n",
    "    'chat_model': CHAT_MODEL, \n",
    "    'stream': True  \n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "for line in response.text.splitlines():\n",
    "    last_message = None\n",
    "    parsed = json.loads(line)\n",
    "    if 'message' in parsed:\n",
    "        last_message = parsed['message']\n",
    "\n",
    "if '</think>' in last_message:\n",
    "    last_message = last_message.split(\"</think>\")[1].strip()\n",
    "\n",
    "export_json(json.loads(last_message), \"eval_lib_pdf.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file (adjust the path if needed)\n",
    "df = pd.read_csv(\"evaluation.csv\", delimiter=\",\")  # or delimiter=';' if your file uses semicolons\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1\n",
      "4.75\n",
      "4.65\n",
      "4.95\n",
      "4.7\n"
     ]
    }
   ],
   "source": [
    "# relevance,correctness,completeness,clarity,depth\n",
    "\n",
    "claude_df = df[df['model'] == 'claude-3.7']\n",
    "print(claude_df['correctness'].mean())\n",
    "print(claude_df['relevance'].mean())\n",
    "print(claude_df['completeness'].mean())\n",
    "print(claude_df['clarity'].mean())\n",
    "print(claude_df['depth'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criteria = ['relevance', 'correctness', 'completeness', 'clarity', 'depth']\n",
    "\n",
    "global_avg_scores = df.groupby('model')[criteria].mean().reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>correctness</th>\n",
       "      <th>completeness</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPT4.1</th>\n",
       "      <td>5.00</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.95</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-3.7</th>\n",
       "      <td>4.75</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.95</td>\n",
       "      <td>4.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            relevance  correctness  completeness  clarity  depth\n",
       "model                                                           \n",
       "GPT4.1           5.00          4.7          4.95     5.00   4.95\n",
       "claude-3.7       4.75          4.1          4.65     4.95   4.70"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute global average across all criteria\n",
    "global_avg_scores['global_average'] = global_avg_scores[criteria].mean(axis=1)\n",
    "\n",
    "# Print results\n",
    "print(global_avg_scores)\n",
    "\n",
    "# Optional: save results to CSV\n",
    "global_avg_scores.to_csv(\"model_global_scores.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
